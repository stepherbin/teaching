{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Jg7JVDh1jlj"
      },
      "source": [
        "# TD : Apprentissage automatique - Approches bayésienne et linéairement discriminantes\n",
        "\n",
        "Répondez aux questions dans le document et copiez les lignes de code qui ont produit les résultats aux endroits indiqués.\n",
        "\n",
        "Les objectifs du TD sont:\n",
        "* Une familiarisation avec la gestion de données.\n",
        "* La mise en oeuvre d'une démarche de conception par apprentissage d'un classifieur sur des problèmes de petite taille (mais parfois de grande dimension).\n",
        "* Une prise en main des ressources logicielles classiques utilisées en Machine Learning et Data Science.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "49SXeP9y5Bmi"
      },
      "source": [
        "______\n",
        "\n",
        "# Introduction: Les ressources logicielles utiles\n",
        "______\n",
        "\n",
        "Dans ce TD, on utilise les ressources de calcul et l'environnement logiciel Python proposés par [Colab](https://colab.research.google.com/).\n",
        "\n",
        "Les données seront accessibles depuis le site [openML](https://www.openml.org/) et seront téléchargées directement depuis ce site.\n",
        "\n",
        "Il sera fait usage de la bibliothèque [scikit-learn](http://scikit-learn.org/stable/modules/classes.html) qui contient la plupart des algorithmes d'apprentissage classiques dans un format unifié.\n",
        "\n",
        "D'autres bibliothèques utiles sont [pandas](https://pandas.pydata.org/) (manipulation de données) et [seaborn](https://seaborn.pydata.org/) (fonctions de visualisation): on en verra quelques exemples au cours du TD.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvzozXj31jlp"
      },
      "source": [
        "______\n",
        "\n",
        "# Partie 1: Les données\n",
        "______\n",
        "\n",
        "On se propose dans cette première partie de construire une démarche d'apprentissage sur un problème simple, et de programmer les algorithmes vus dans le cours (approches discriminantes et génératives).\n",
        "\n",
        "Chaque jeu de données est constitué de quatre ensembles :\n",
        "* Un ensemble de données test utilisées pour l'apprentissage : X_train\n",
        "* Un ensemble de classes associées aux données d'apprentissage : y_train\n",
        "* Un ensemble de données à tester : X_test\n",
        "* Un ensemble de classes associées aux données de test : y_test\n",
        "\n",
        "Les données de test ne doivent pas être utilisées pour l'apprentissage proprement dit, seulement pour son évaluation.\n",
        "\n",
        "Remarque: Pour visualiser dans le notebook, appliquer l'instruction:\n",
        ">```python\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "après chaque fonction de visualisation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXY0yzAO1jls"
      },
      "source": [
        "## Familiarisation avec les données\n",
        "\n",
        "On se propose de réaliser une classification binaire d'un ensemble de données en utilisant.\n",
        "\n",
        "Le problème est une classification binaire pour détecter des billets de banque frauduleux. Les données source d'origine sont accessibles [ici](https://www.openml.org/search?type=data&status=active&id=1462).\n",
        "\n",
        "L'objectif est de dérouler les différentes étapes d'une démarche d'apprentissage automatique:\n",
        "1. Constitution de la base d'apprentissage\n",
        "2. Visualisation et analyse des données\n",
        "3. Choix d'une approche\n",
        "4. Optimisation (apprentissage des paramètres du prédicteur)\n",
        "5. Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2pnNyoW35Wc8"
      },
      "source": [
        "### Base d'apprentissage (10min)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xs5nFUr51jlx"
      },
      "source": [
        "\n",
        "Exécuter le code ci-dessous qui charge les données, et qui les répartit en ensemble d'apprentissage et de test (fonction 'train_test_split')\n",
        "\n",
        "Montrer les tailles des données (nombre de caractéristiques, nombre de données).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "4wIXBDydCd6h"
      },
      "outputs": [],
      "source": [
        "# Librairies scientifique et visualisation utiles standard\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MN6fhLhP1jl0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cad55ee5-55af-4649-ef5d-f426a9055d4d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Les données sont de dimension 45\n",
            "Le nombre de données d'apprentissage est -1\n"
          ]
        }
      ],
      "source": [
        "# Pour manipuler les données\n",
        "import pandas as pd\n",
        "\n",
        "# Pour séparer les données en apprentissage et test\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Pour visualiser  les données\n",
        "def show_data_2D(X,Y):\n",
        "    np.unique(Y)\n",
        "    fig, ax = plt.subplots(figsize=(8, 6))\n",
        "    for id in np.unique(Y):\n",
        "        idpositive=np.nonzero(Y == id)[0]\n",
        "        ax.scatter(X[idpositive,0], X[idpositive,1], s=50)\n",
        "\n",
        "# Chargement des données et constitution de la base d'apprentissage\n",
        "url = 'https://raw.githubusercontent.com/stepherbin/teaching/refs/heads/master/IOGS/data/data_banknote_authentication.txt'\n",
        "data_all = pd.read_csv(url)\n",
        "\n",
        "y = data_all.to_numpy()[:,-1]\n",
        "X = data_all.to_numpy()[:,:-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.8)\n",
        "\n",
        "# Données d'apprentissage réparties en fonction des classes\n",
        "X0 = X_train[y_train == 0,:]\n",
        "X1 = X_train[y_train == 1,:]\n",
        "\n",
        "##################\n",
        "# MODIFIER LE CODE\n",
        "ndim = 45\n",
        "ntrain = -1\n",
        "#################\n",
        "\n",
        "print(\"Les données sont de dimension {:d}\".format(ndim))\n",
        "print(\"Le nombre de données d'apprentissage est {:d}\".format(ntrain))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zfA9d9xR1jmB"
      },
      "source": [
        "### Visualisation des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swvXzFRX1jmF"
      },
      "source": [
        "Jouer avec les visualisation des distributions de données en modifiant des paramètres du code ci-dessous et en éxécutant la cellule.\n",
        "\n",
        "Vous pourrez utiliser les fonctions de visualisation \"show_data_2D\" pour mettre en évidence les corrélations, et [plt.hist](https://matplotlib.org/devdocs/api/_as_gen/matplotlib.pyplot.hist.html) pour visualiser des histogrammes pour chaque classe comme dans l'exemple.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T4TOisi1jmI"
      },
      "outputs": [],
      "source": [
        "# Dimensions à étudier (à répéter pour d'autres dimensions)\n",
        "feat1 = 0\n",
        "feat2 = 3\n",
        "\n",
        "# Distributions 2D\n",
        "show_data_2D(X_train[:,[feat1, feat2]], y_train)\n",
        "plt.show()\n",
        "\n",
        "# Visualise les distributions pour un attribut\n",
        "a0 = X0[:,feat1]\n",
        "a1 = X1[:,feat1]\n",
        "\n",
        "# Histogrammes des valeurs d'attributs pour chacune des classes\n",
        "plt.hist(a0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(a1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ob5RPbhfrcM9"
      },
      "source": [
        "### Autre manière de visualiser directement les données (en utilisant seaborn+pandas)\n",
        "\n",
        "[Seaborn](https://seaborn.pydata.org/index.html) est une bibliothèque python très complète de visualisation de données. Elle contient des fonctions sophistiquées de rendu (exemple ci-dessous).\n",
        "\n",
        "\n",
        "[Pandas](https://pandas.pydata.org/) est une bibliothèque de gestion et de manipulation de données. La structure de référence est le [*DataFrame*](https://pandas.pydata.org/docs/reference/frame.html) qui organise les données en champs indéxés par des étiquettes.\n",
        "\n",
        "A partir des différents diagrammes, identifier les attributs individuels les plus informatifs pour la classification."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lKj4QPpDrbPc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "\n",
        "df = pd.DataFrame(X, columns=names[:-1])\n",
        "df['class'] = np.int64(y)\n",
        "\n",
        "df.info()\n",
        "\n",
        "sns.histplot(data=df, x='V1', hue='class', stat='probability')\n",
        "\n",
        "sns.pairplot(df, hue='class',diag_kind=\"hist\",corner=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T8YHzs0-3FLw"
      },
      "source": [
        "## Construire une \"baseline\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_I27D6X1iSV"
      },
      "source": [
        "### Un premier essai de classification: chercher la meilleure caractéristique\n",
        "\n",
        "* Modélisation des distributions 1D\n",
        "* Construire un prédicteur pour chaque dimension\n",
        "* Trouver la meilleure dimension à discriminer pour définir une \"baseline\" (= un algorithme simple de référence)\n",
        "\n",
        "Vous pouvez regarder la bibliothèque [scipy.stats](https://docs.scipy.org/doc/scipy/reference/stats.html) qui définit les familles courantes de distributions statistiques.\n",
        "\n",
        "Essayez d'estimer quelques lois paramétriques pertinentes (méthode 'fit' des classes de scipy.stats) en les superposant aux histogrammes des données à parir du code ci-dessous. Jouer sur les différents paramètres pour visualiser les bonnes et mauvaises distributions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hpqUPCMu1jmX",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Choix de la distribution\n",
        "from scipy.stats import laplace, expon, norm, gamma, beta, cauchy, logistic\n",
        "\n",
        "# Définit le type de distribution\n",
        "distrib=norm\n",
        "\n",
        "# Choix de la caractéristique\n",
        "feat1 = 0\n",
        "a0 = X0[:,feat1]\n",
        "a1 = X1[:,feat1]\n",
        "\n",
        "# Visualise les distributions pour une caractéristique\n",
        "plt.clf()\n",
        "\n",
        "data = a0\n",
        "param = distrib.fit(data)\n",
        "\n",
        "# Visualise l'histogramme\n",
        "plt.hist(data, bins=25, density=True, alpha=0.6, color='b')\n",
        "\n",
        "# Visualise la PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = distrib.pdf(x, *param)\n",
        "plt.plot(x, p, 'b', linewidth=2)\n",
        "\n",
        "data = a1\n",
        "param = distrib.fit(data)\n",
        "\n",
        "# Visualise l'histogramme\n",
        "plt.hist(data, bins=25, density=True, alpha=0.6, color='g')\n",
        "\n",
        "# Visualise la PDF.\n",
        "xmin, xmax = plt.xlim()\n",
        "x = np.linspace(xmin, xmax, 100)\n",
        "p = distrib.pdf(x, *param)\n",
        "plt.plot(x, p, 'g', linewidth=2)\n",
        "\n",
        "title = \"Fit results for %s\" % distrib.name\n",
        "plt.title(title)\n",
        "\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RV_89KuH9M6g"
      },
      "source": [
        "Une fois le type de distribution choisi, construire une étape de classification simple. Par exemple, en calculant le signe du log-rapport des probabilités a posteriori: $\\log(P[y=1|x]) - \\log(P[y=0|x])$ en appliquant la loi de Bayes.\n",
        "\n",
        "Pour rappel, la loi de Bayes est:\n",
        "$$P[y|x] = \\frac{P[x|y].P[y]}{P[x]}$$\n",
        "\n",
        "Les log-vraisemblances conditionnelles $P[x|y]$ seront calculées en estimant les paramètres des modèles de distribution (cf. code précédent) et en appliquant la fonction `logpdf`sur la distribution.\n",
        "\n",
        "La prise en compte des lois a priori $P[y]$ peut également être réalisée en calculant directement le ratio $\\log \\frac{P[y=1]}{P[y=0]} = \\log \\frac{|C_1|}{|C_2|}$ où $|C_k|$ est le nombre d'échantillons de la classe $k$ dans la base d'apprentissage.\n",
        "\n",
        "Coder les étapes permettant d'estimer les paramètres des lois modélisant les distributions d'attributs, de construire le prédicteur à partir de ces lois, et d'évaluer la qualité du prédicteur.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z5O6b3zB9NFh"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "# Calcul des log prior\n",
        "logprior = math.log((y_train==0).sum()/(y_train==1).sum())\n",
        "\n",
        "# Choix de la caractéristique\n",
        "feat1 = 0\n",
        "\n",
        "# Données d'apprentissage\n",
        "a0 = X0[:,feat1]\n",
        "a1 = X1[:,feat1]\n",
        "\n",
        "# Données de test\n",
        "at = X_test[:, feat1]\n",
        "\n",
        "distrib = norm # ou une autre distribution\n",
        "\n",
        "# Estimation des paramètres des lois et prédiction sur données de test\n",
        "# les prédictions sont dans y_pred\n",
        "# les valeurs dans l'espace de décision (log ratio probab) sont dans z_pred\n",
        "#########################\n",
        "## METTRE VOTRE CODE ICI\n",
        "\n",
        "##########################\n",
        "\n",
        "# Calcul de l'erreur de prédiction (comparaison avec la vérité terrain y_test)\n",
        "erreur = (y_pred != y_test).sum() / y_test.size\n",
        "\n",
        "print(\"L'erreur de prédiction est {:.1f}%\".format(100*erreur))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gKc7ZNggRuxz"
      },
      "source": [
        "**Visualisation des log probabibilités**\n",
        "\n",
        "On visualise les distributions des log probabilités par des histogrammes pour rendre compte de la capacité du prédicteur à les discriminer.\n",
        "\n",
        "ATTENTION: on ne visualise pas la répsrtition des données dans l'espace des caractéristiques, mais leur projection dans l'espace de décision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mhi8IAWpKf7u"
      },
      "outputs": [],
      "source": [
        "# Si z_pred est la valeur du log ratio des probabilités a posteriori (à calculer)\n",
        "# on visualise ici les distributions de sa valeur pour les deux classes\n",
        "\n",
        "Z0 = z_pred[y_test == 0]\n",
        "Z1 = z_pred[y_test == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUx3oY_2OVvP"
      },
      "source": [
        "Quelles sont les performances d'un tel classifieur? Pourquoi sont-elles faibles?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TY7AoQo31uhv"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "# Partie 2: Comparaison des approches génératives et discriminantes\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "Dans cette partie, nous allons tester différentes approches d'apprentissage qui prennent en compte la nature multivariée des données. On a vu en effet dans la section précédente que chaque attribut ou caractéristique pris individuellement était peu discriminant.\n",
        "\n",
        "Deux types d'approches permettent de construire des prédicteurs multi-variés: génératives, qui s'appuient sur une modélisation des données, et discriminantes, qui estiment directement les paramètres du prédicteur.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v9rKbf5VJaaf"
      },
      "source": [
        "## Modèles génératifs\n",
        "\n",
        "Nous allons tester deux types de modèles génératifs qui s'appuient sur une estimation des probabilités a posteriori pour construire la fonction de prédiction: la modélisation gaussienne multivariée, et l'approchche bayésienne naïve."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBLA1f1O-JPN"
      },
      "source": [
        "### Modélisation gaussienne multi-variée\n",
        "\n",
        "Dans cette approche, l'objectif est d'estimer les paramètres d'un modèle gaussien multi-varié à partir des données d'apprentissage, et ensuite de comparer les probabilités a posteriori.\n",
        "\n",
        "Le modèle Gaussien multi-varié est défini par la fonction [`multivariate_normal`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.multivariate_normal.html#scipy.stats.multivariate_normal) et prend comme arguments les moyennes et covariance qu'il faut estimer par ailleurs, par exemple en utilisant les fonctions numpy [`mean`](https://numpy.org/doc/stable/reference/generated/numpy.mean.html) et [`cov`](https://numpy.org/doc/stable/reference/generated/numpy.cov.html).\n",
        "\n",
        "D'autres méthodes plus robustes peuvent être utilisées pour estimer la [matrice de covariance](https://scikit-learn.org/stable/modules/covariance.html#covariance).\n",
        "\n",
        "Remarquer que le modèle `multivariate_normal` permet d'estimer directement le logarithme de la densité de probabilité (`logpdf`) pour pouvoir ensuite comparer le signe de la différence des log-probabilités pour décider.\n",
        "\n",
        "Quelles sont les performances de cette approche?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hx-8BzQ7-JZu"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Calcul des log prior\n",
        "logprior = math.log((y_train==0).sum()/(y_train==1).sum())\n",
        "\n",
        "# Calcul des moyennes et covariances\n",
        "cc0=np.cov(np.transpose(X0))\n",
        "mm0=np.mean(X0,axis=0)\n",
        "\n",
        "cc1=np.cov(np.transpose(X1))\n",
        "mm1=np.mean(X1,axis=0)\n",
        "\n",
        "# Apprentissage des lois et prédiction sur données de test\n",
        "# les prédictions sont dans y_pred\n",
        "# les valeurs dans l'espace de décision sont dans z_pred\n",
        "#########################\n",
        "### METTRE VOTRE CODE ICI\n",
        "\n",
        "\n",
        "############################\n",
        "\n",
        "# Evaluation\n",
        "erreur = (y_pred != y_test).sum() / y_test.size\n",
        "\n",
        "print(\"Gaussien multivarié: L'erreur de prédiction est {:.1f}%\".format(100*erreur))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vZxUzEVhNh-V"
      },
      "outputs": [],
      "source": [
        "# Visualisation de la valeur de la fonction de décision\n",
        "\n",
        "Z0 = z_pred[y_test==0]\n",
        "Z1 = z_pred[y_test==1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWO5BekW1jmQ"
      },
      "source": [
        "### Bayésien naïf\n",
        "\n",
        "On se propose à partir de maintenant d'utiliser la librarie [scikit-learn](http://scikit-learn.org/stable/modules/classes.html) qui contient un grand nombre d'algorithmes d'apprentissage et une interface \"orientée objet\" facile d'utilisation.\n",
        "\n",
        "Toutes les méthodes contiennent les fonctions suivantes:\n",
        "* `fit`: pour apprendre/estimer les paramètres du modèle.\n",
        "* `predict`: pour prédire (classe, valeur) à partir du modèle appris.\n",
        "* `score`: pour calculer les performances du prédicteur.\n",
        "\n",
        "\n",
        "Le principe du bayésien naïf est d'estimer la loi conditionnelle $P[x_i | y]$ pour les différentes hypothèses $y\\in\\{0,1\\}$ et chaque caractéristique $x_i$ et d'appliquer ensuite l'inversion bayésienne avec hypothèse d'indépendance pour calculer la probabilité (ou log probabilité) a posteriori pour chacune des classes $y$ et une donnée à prédire $\\mathbf{x}$.\n",
        "\n",
        "$$\\log P[y|\\mathbf{x}] \\propto \\sum_{i=1}^d \\log P[x_i|y] + \\log P[y]$$\n",
        "\n",
        "L'apprentissage consiste donc à estimer un bon modèle de la distribution de chaque caractéristique (binomiale, multinomiale, gaussienne, histogram, mélange de gaussienne...)\n",
        "\n",
        "Regarder les différents modèles [\"bayésiens naïfs\"](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.naive_bayes) de scikit-learn et déterminer la bonne classe à appliquer, et si elle est applicable, en fonction de l'allure des distributions de caractéristiques étudiées dans la partie 1.\n",
        "\n",
        "Compléter le code ci-dessous pour vérifier le choix du modèle, et ses paramètres éventuels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1mURrm6h4wsm"
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import GaussianNB # code à compléter\n",
        "\n",
        "#Coder les étapes d'apprentissage et de prédiction sur les données de test\n",
        "# les prédictions sont dans y_pred\n",
        "# les valeurs dans l'espace de décision sont dans z_pred\n",
        "########################\n",
        "## METTRE VOTRE CODE ICI\n",
        "\n",
        "##########################\n",
        "\n",
        "# Evaluation de l'erreur de la prédiction y_pred\n",
        "erreur = (y_pred != y_test).sum() / y_test.size\n",
        "\n",
        "print(\"Bayésien naïf: L'erreur de prédiction est {:.2f}%\".format(100*erreur))\n",
        "\n",
        "# Autre manière de calculer l'erreur directement (fonction score = predict + evaluation)\n",
        "print(\"Bayésien naïf: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jN2KN9WXLIXX"
      },
      "outputs": [],
      "source": [
        "#Visualisation du log du ratio des probabilités selon les classes\n",
        "\n",
        "Z0 = z_pred[y_test == 0]\n",
        "Z1 = z_pred[y_test == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MSvEHlor1jmg"
      },
      "source": [
        "\n",
        "<mark>QUESTIONS:</mark>\n",
        "\n",
        "* Comparer les performances du Bayésien naïf et des classifieurs de caractéristiques?\n",
        "\n",
        "* Comment expliquer la différence de performance entre les deux approches (Gaussien multivarié et Bayésien Naïf)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BvWEr0FOIymW"
      },
      "source": [
        "## Modèles discriminants\n",
        "\n",
        "On a vu dans le cours trois manière d'estimer un modèle discriminant **linéaire**:\n",
        "\n",
        "* moindres carrés\n",
        "* régression logistique\n",
        "* analyse discriminante avec critère de Fisher\n",
        "\n",
        "On va les étudier dans la suite de cette section."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4lpk3AgjKbbp"
      },
      "source": [
        "### Moindres carrés\n",
        "\n",
        "Le code ci-dessous construit la fonction de prédiction en utilisant la fonction de régression linéaire [`linear_regression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html).\n",
        "\n",
        "Reportez-vous au cours pour analyser le rôle des différents éléments de code. Comment est réalisée la prédiction?\n",
        "\n",
        "Les performances du prédicteur peuvent-elles être améliorées? Comparer la distribution des données avec la manière dont est prise la décision.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nk3P139mKjxw"
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model\n",
        "\n",
        "reg = linear_model.LinearRegression()\n",
        "\n",
        "# Augmentation de la dimension des données pour estimer le biais\n",
        "X_train1 = np.hstack((X_train,np.ones((X_train.shape[0], 1)) ))\n",
        "X_test1 = np.hstack((X_test,np.ones((X_test.shape[0], 1)) ))\n",
        "\n",
        "# Apprentissage\n",
        "reg.fit(X_train1, y_train)\n",
        "w_reg = reg.coef_\n",
        "\n",
        "# Prédiction\n",
        "z_pred = reg.predict(X_test1)\n",
        "y_pred = (z_pred > 0.5).astype(int)\n",
        "\n",
        "# Evaluation\n",
        "erreur = (y_pred != y_test).sum() / y_test.size\n",
        "\n",
        "print(\"Moindres carrés: L'erreur de prédiction est {:.2f}%\".format(100*erreur))\n",
        "\n",
        "# Visualisation des distributions de données d'apprentissage projetées sur la direction discriminante\n",
        "Z0 = z_pred[y_test == 0]\n",
        "Z1 = z_pred[y_test == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbLEtI--Kxff"
      },
      "source": [
        "### Régression logistique\n",
        "\n",
        "La régression logistique consiste à estimer directement la loi a posteriori par une forme linéaire. Ses paramètres peuvent être calculés en utilisant la fonction\n",
        "[`LogisticRregression`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) de scikit-learn.\n",
        "\n",
        "Construire le prédicteur, estimer ses performances et visualiser la distribution des données dans l'espace de décision.\n",
        "\n",
        "Dans scikit-learn, on peut accéder à l'espace de décision (avant prédiction) par `decision_function` (pour générer `z_pred`).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AiuzuhnDyHPa"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Apprentissage, prédiction et évaluation\n",
        "#######################\n",
        "# METTRE VOTRE CODE ICI\n",
        "\n",
        "############################\n",
        "\n",
        "Z0 = z_pred[y_train == 0]\n",
        "Z1 = z_pred[y_train == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZWotbks3KkEO"
      },
      "source": [
        "### Analyse discriminante (Fisher)\n",
        "\n",
        "L'analyse discriminante utilisant le critère de contraste de Fisher pour un problème à deux classes consiste à calculer la direction discriminante $\\mathbf{w}$ selon\n",
        "$$\\mathbf{w} = S_I^{-1}.(\\mathbf{\\mu}_1 - \\mathbf{\\mu}_0)$$\n",
        "où $\\mu_k$ est la moyenne des données pour la classe $k$ et la matrice $S_I$ mesure la disparité intra-classe et vaut $\\Sigma_0 + \\Sigma_1$, la somme des matrices de covariance de chacune des classes.\n",
        "\n",
        "Calculer la direction discriminante et visualiser les histogrammes des données projetées. Déterminer à partir de la visualisation un seuil de détection (le biais) pour discrirminer classe 0 vs. classe 1 et calculer l'erreur de prédiction.\n",
        "\n",
        "Rappel: le produit matriciel peut se calculer sous numpy par la fonction [`dot`](https://numpy.org/doc/stable/reference/generated/numpy.dot.html), l'inversion par [`inv`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.inv.html) et le calule de la norme par [`norm`](https://numpy.org/doc/stable/reference/generated/numpy.linalg.norm.html)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0j_dnN8Cdbeg"
      },
      "outputs": [],
      "source": [
        "from numpy.linalg import inv, norm\n",
        "\n",
        "cc0=np.cov(np.transpose(X0))\n",
        "mm0=np.mean(X0,axis=0)\n",
        "\n",
        "cc1=np.cov(np.transpose(X1))\n",
        "mm1=np.mean(X1,axis=0)\n",
        "\n",
        "# Apprentissage (calcul de la direction discriminante)\n",
        "#######################\n",
        "# METTRE VOTRE CODE ICI\n",
        "\n",
        "#######################################\n",
        "\n",
        "# Prédiction (w est la direction, b est le biais)\n",
        "z_pred = np.dot(X_test, w) - b\n",
        "y_pred = (z_pred>0).astype(int)\n",
        "\n",
        "print(\"Fisher LDA: L'erreur de prédiction est {:.1f}%\".format(100*(y_pred != y_test).sum() / y_test.size))\n",
        "\n",
        "Z0 = z_pred[y_test == 0]\n",
        "Z1 = z_pred[y_test == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZvCkq1TMyEr6"
      },
      "source": [
        "Une autre manière d'implémenter une analyse discriminante linéaire est d'utiliser la fonction  [`LinearDiscriminantAnalysis`](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html)de scikit-learn. Cette version fait l'hypothèse que les matrices de covariance pour les deux classes sont identiques.\n",
        "\n",
        "On peut montrer que sous cette hypothèse, la direction discriminante est la même que celle obtenue par moindre carré: vérifier le empiriquement. La direction discriminante est accessible comme une variable `coef_` du modèle.\n",
        "\n",
        "Inspirez-vous des code précédents pour estimer le modèle, faire la prédiction, et visualiser les distributions dans l'espace de décision."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mnheH5_sKxTA"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "\n",
        "# Apprentissage, prédiction et évaluation et comparaison des résultats\n",
        "# avec ceux du prédicteur aux moindres carrés\n",
        "#######################\n",
        "# METTRE VOTRE CODE ICI\n",
        "\n",
        "\n",
        "###########################\n",
        "\n",
        "Z0 = z_pred[y_test == 0]\n",
        "Z1 = z_pred[y_test == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mlie85JM3d6_"
      },
      "source": [
        "# Partie 3: Autres jeux de données\n",
        "\n",
        "On se propose dans cette partie de comparer les comportements des prédicteurs étudiés précédemment sur d'autres jeux de données."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iV3k-P-8_Qv7"
      },
      "source": [
        "## Un dataset de taille intermédiaire\n",
        "\n",
        "Ce [jeu de données](https://www.openml.org/search?type=data&sort=runs&id=1471&status=active) est un peu plus grand que le précédent (détection de faux billets) mais de plus grande taille. Il a pour objectif de prédire à partir de signaux d'électro-encéphalogramme si les yeux sont ouverts ou fermés.\n",
        "\n",
        "On cherchera donc encore ici à réaliser une classification parmi deux hypothèses.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZfpfnFFQ4ABs"
      },
      "outputs": [],
      "source": [
        "# Téléchargement des données\n",
        "url = 'https://raw.githubusercontent.com/stepherbin/teaching/refs/heads/master/IOGS/data/eeg-eye-state.csv'\n",
        "data_all = pd.read_csv(url)\n",
        "\n",
        "y = data_all.to_numpy()[:,-1]\n",
        "X = data_all.to_numpy()[:,:-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X,y,\n",
        "                                                    random_state=42,\n",
        "                                                    stratify=y,\n",
        "                                                    test_size=0.8)\n",
        "\n",
        "\n",
        "X0 = X_train[y_train == 0,:]\n",
        "X1 = X_train[y_train == 1,:]\n",
        "\n",
        "print(\"Les données sont de dimension {:d}\".format(X_train.shape[1]))\n",
        "print(\"Le nombre de données d'apprentissage est {:d}\".format(y_train.shape[0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1YY-5z5qaVv"
      },
      "source": [
        "### Visualisation des données\n",
        "\n",
        "Regarder et visualiser les données sur quelques dimensions pour se faire une idée de leur distribution. On pourra faire une boucle sur les caractéristiques pour comparer les histogrammes, et regarder les corrélations des caractéristiques.\n",
        "\n",
        "Qu'en conclure sur la difficulté du problème? Quels sont les approches potentiellement intéressantes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_WgpG021qUFM"
      },
      "outputs": [],
      "source": [
        "# Visualisation des données\n",
        "#######################################\n",
        "## METTRE VOTRE CODE ICI\n",
        "\n",
        "####################################"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-s1ISclYyWb"
      },
      "source": [
        "### Analyse discriminante\n",
        "\n",
        "Reprendre les codes de la section précédente, et estimer les performances du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "waKkK-C76WKE"
      },
      "outputs": [],
      "source": [
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n",
        "z_pred = clf.decision_function(X_train)\n",
        "\n",
        "Z0 = z_pred[y_train == 0]\n",
        "Z1 = z_pred[y_train == 1]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZfTp8-VY4dg"
      },
      "source": [
        "### Gaussien multivarié\n",
        "\n",
        "Reprendre les codes de la section précédente, et estimer les performances du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r9Q8wfuYJ4Rm"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "X0 = X_train[y_train == 0,:]\n",
        "X1 = X_train[y_train == 1,:]\n",
        "\n",
        "cc0=np.cov(np.transpose(X0))\n",
        "mm0=np.mean(X0,axis=0)\n",
        "d0 = multivariate_normal(mean=mm0, cov=cc0)\n",
        "\n",
        "cc1=np.cov(np.transpose(X1))\n",
        "mm1=np.mean(X1,axis=0)\n",
        "d1 = multivariate_normal(mean=mm1, cov=cc1)\n",
        "\n",
        "lk0=d0.logpdf(X_test)\n",
        "lk1=d1.logpdf(X_test)\n",
        "\n",
        "y_pred = (lk1 - lk0 > 0).astype('int')\n",
        "\n",
        "print(\"Gaussien multivarié: L'erreur de prédiction est {:.1f}%\".format(100*(y_pred != y_test).sum() / y_test.size))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0uzesQtWBOCA"
      },
      "outputs": [],
      "source": [
        "z_pred = lk1 - lk0\n",
        "\n",
        "Z0 = z_pred[y_test==0]\n",
        "Z1 = z_pred[y_test==1]\n",
        "\n",
        "\n",
        "# Pour limiter l'étendu de la visualisation (existence de valeurs très grandes ou très petites) on filtre les quantiles extrémes.\n",
        "from scipy.stats.mstats import mquantiles\n",
        "\n",
        "Z0_quant = mquantiles(Z0, prob=[0.01, 0.99])\n",
        "Z0 = Z0[Z0 > Z0_quant[0]]\n",
        "Z0 = Z0[Z0 < Z0_quant[1]]\n",
        "\n",
        "Z1_quant = mquantiles(Z1, prob=[0.01, 0.99])\n",
        "Z1 = Z1[Z1 > Z1_quant[0]]\n",
        "Z1 = Z1[Z1 < Z1_quant[1]]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lMirIzMD0i56"
      },
      "source": [
        "### Régression logistique\n",
        "\n",
        "Reprendre les codes de la section précédente, et estimer les performances du modèle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vun0zNMS0i5_"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='liblinear', C=100000).fit(X_train, y_train)\n",
        "print(\"Régression logisitique: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n",
        "z_pred = clf.decision_function(X_test)\n",
        "\n",
        "Z0 = z_pred[y_test==0]\n",
        "Z1 = z_pred[y_test==1]\n",
        "\n",
        "from scipy.stats.mstats import mquantiles\n",
        "\n",
        "Z0_quant = mquantiles(Z0, prob=[0.01, 0.99])\n",
        "Z0 = Z0[Z0 > Z0_quant[0]]\n",
        "Z0 = Z0[Z0 < Z0_quant[1]]\n",
        "\n",
        "Z1_quant = mquantiles(Z1, prob=[0.01, 0.99])\n",
        "Z1 = Z1[Z1 > Z1_quant[0]]\n",
        "Z1 = Z1[Z1 < Z1_quant[1]]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abZuZ98kF--A"
      },
      "source": [
        "<mark>QUESTION</mark>\n",
        "* Quel modèle semble le plus intéressant pour ce problème?\n",
        "* Avez-vous une explication de la différence de comportement des approches?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bLhHp_Qfo6BK"
      },
      "source": [
        "### Augmentation de caractéristiques + LDA\n",
        "\n",
        "Dans cette partie, on se propose d'augmenter l'expressivité des caractéristiques en rajoutant des moments d'ordre 2 dans la représentation. Cela peut se faire en utilisant la fonction [`PolynomialFeatures`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html).\n",
        "\n",
        "La logique derrière cette augmentation de représentation est de construire artificiellement une surface de décision plus complexe mais en gardant un mécanisme d'estimation s'appuyant sur des techniques linéaires.\n",
        "\n",
        "On construit une fonction $\\Phi: \\mathbf{x} \\mapsto \\Phi(\\mathbf{x}) \\in R^m$ qui transforme les données d'origine dans $R^d$ dans une espace $R^m$ où, en général, $m > n$. Par exemple, on va construire des produits de chaque caractéristique $\\Phi_k(\\mathbf{x}) = x_i.x_j$. La direction de décision $\\mathbf{w}$ sera alors à rechercher dans $R^m$ et la fonction de décision:\n",
        "$$z(\\mathbf{x}) = w_0 + w_1.\\Phi_1(\\mathbf{x}) + \\cdots w_k.\\Phi_k(\\mathbf{x})$$\n",
        "Si l'on augmente ainsi les représentations, on peut générer des surfaces de décision quadratiques.\n",
        "\n",
        "Utiliser cette démarche pour construire une fonction de décision quadratique en augmentant les représentations avec la fonction `PolynomialFeatures`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jUYM2lk646pt"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "poly = PolynomialFeatures(2)\n",
        "\n",
        "Xp_train = poly.fit_transform(X_train)\n",
        "Xp_test = poly.fit_transform(X_test)\n",
        "\n",
        "# Apprentissage et calcul de z_pred (espace de décision)\n",
        "######################################\n",
        "## METTRE VOTRE CODE ICI\n",
        "\n",
        "clf = LinearDiscriminantAnalysis()\n",
        "clf.fit(Xp_train, y_train)\n",
        "\n",
        "z_pred = clf.decision_function(Xp_test)\n",
        "\n",
        "#####################################\n",
        "\n",
        "Z0 = z_pred[y_test==0]\n",
        "Z1 = z_pred[y_test==1]\n",
        "\n",
        "from scipy.stats.mstats import mquantiles\n",
        "\n",
        "Z0_quant = mquantiles(Z0, prob=[0.001, 0.999])\n",
        "Z0 = Z0[Z0 > Z0_quant[0]]\n",
        "Z0 = Z0[Z0 < Z0_quant[1]]\n",
        "\n",
        "Z1_quant = mquantiles(Z1, prob=[0.001, 0.999])\n",
        "Z1 = Z1[Z1 > Z1_quant[0]]\n",
        "Z1 = Z1[Z1 < Z1_quant[1]]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n",
        "\n",
        "print(\"L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(Xp_test, y_test))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-t-lpN_Y1M5l"
      },
      "source": [
        "### Augmentation de caractéristiques + Régression logistique\n",
        "\n",
        "Même principe mais avec régression logistique. Ici, l'apprentissage de la fonction a nécessité de normaliser les données pour assurer la convergence de l'optimisation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "arLAIMqM1NC7"
      },
      "outputs": [],
      "source": [
        "poly = PolynomialFeatures(2)\n",
        "\n",
        "Xp_train = poly.fit_transform(X_train)\n",
        "Xp_test = poly.fit_transform(X_test)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, max_iter=1000, solver='liblinear', C=10, penalty='l2')\n",
        "clf.fit(Xp_train, y_train)\n",
        "\n",
        "z_pred = clf.decision_function(Xp_test)\n",
        "\n",
        "print(\"Régression logisitique: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(Xp_test, y_test))))\n",
        "\n",
        "\n",
        "from sklearn import preprocessing\n",
        "scaler = preprocessing.StandardScaler().fit(Xp_train)\n",
        "\n",
        "Xp_scaled = scaler.transform(Xp_train)\n",
        "Xp_test_scaled = scaler.transform(Xp_test)\n",
        "\n",
        "clf = LogisticRegression(random_state=0, max_iter=1000, solver='liblinear', C=10000, penalty='l2')\n",
        "clf.fit(Xp_scaled, y_train)\n",
        "\n",
        "z_pred = clf.decision_function(Xp_test_scaled)\n",
        "\n",
        "print(\"Régression logisitique + augmentation: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(Xp_test_scaled, y_test))))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aCV7n67bHuwQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "Z0 = z_pred[y_test==0]\n",
        "Z1 = z_pred[y_test==1]\n",
        "\n",
        "from scipy.stats.mstats import mquantiles\n",
        "\n",
        "Z0_quant = mquantiles(Z0, prob=[0.01, 0.99])\n",
        "Z0 = Z0[Z0 > Z0_quant[0]]\n",
        "Z0 = Z0[Z0 < Z0_quant[1]]\n",
        "\n",
        "Z1_quant = mquantiles(Z1, prob=[0.01, 0.99])\n",
        "Z1 = Z1[Z1 > Z1_quant[0]]\n",
        "Z1 = Z1[Z1 < Z1_quant[1]]\n",
        "\n",
        "plt.hist(Z0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(Z1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kbh1X0tV3TNW"
      },
      "source": [
        "### SVM (pour plus tard)\n",
        "\n",
        "Les Support Vector Machines permettent de mieux contrôler la surface discrimante et sa forme. On les étudiera dans le cours N°4.\n",
        "\n",
        "Ici, ils donnent d'assez bonnes performances sur ce problème très mélangé."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HF-I7rEQ9QDH"
      },
      "outputs": [],
      "source": [
        "from sklearn import svm\n",
        "\n",
        "clf = svm.SVC(shrinking=True,max_iter=1000000,gamma=1e-3, C=3.5, kernel='rbf')\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "print(\"SVM: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n",
        "z_pred = clf.decision_function(X_test)\n",
        "\n",
        "a0 = z_pred[y_test==0]\n",
        "a1 = z_pred[y_test==1]\n",
        "\n",
        "plt.hist(a0, color='b', alpha=0.5, bins=20, label=['class 0'])\n",
        "plt.hist(a1, color='g', alpha=0.5, bins=20, label=['class 1'])\n",
        "plt.legend(loc='upper right')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6T_J-8v93TXB"
      },
      "outputs": [],
      "source": [
        "# Pour rechercher par validation croisée les bons paramètres\n",
        "\n",
        "# # Import de la classe pour Validation Croisée\n",
        "# from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# svc = svm.SVC(shrinking=True,max_iter=1000000,gamma='auto') # max_iter = 1000000 pour limiter les non convergences de l'optimiseur\n",
        "\n",
        "# parameters = [{'kernel':['rbf'],\n",
        "#               'C':np.logspace(-2, 2, 7),\n",
        "#               'gamma':np.logspace(-3, 2, 10)},\n",
        "#               {'kernel':['poly'],\n",
        "#               'C':np.logspace(-2, 2, 7),\n",
        "#               'degree':[2,3,4]}]\n",
        "\n",
        "# parameters = {'kernel':['rbf'],\n",
        "#               'C':np.logspace(0, 1, 7),\n",
        "#               'gamma':np.logspace(-4, -2, 10)}\n",
        "\n",
        "# clf = GridSearchCV(svc, parameters, cv=2)\n",
        "# clf.fit(X_train, y_train)\n",
        "\n",
        "# bestsvc = clf.best_estimator_\n",
        "\n",
        "# print(\"Best score = {:.2f}% with kernel {}\".format(100*(1-clf.best_score_), bestsvc.kernel))\n",
        "# if bestsvc.kernel == 'rbf':\n",
        "#     print(\"Parameters are gamma = {:.2g} and C = {:2g}\".format(bestsvc.gamma, bestsvc.C))\n",
        "# elif bestsvc.kernel == 'poly':\n",
        "#     print(\"Parameters are degree = {} and C = {:2g}\".format(bestsvc.degree, bestsvc.C))\n",
        "\n",
        "# print(\"SVM: L'erreur de prédiction est {:.2f}%\".format(100*(1-bestsvc.score(X_test, y_test))))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OhdEPQfQ_XO4"
      },
      "source": [
        "## Un dataset de très grande dimension"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_l9NeUP4_NMz"
      },
      "source": [
        "Le nouveau problème d'apprentissage est une classification binaire de textes en provenance de newsgroups, c'est-à-dire des forums de discussion sur une certaine thématique. L'objectif de la classification est de prédire le groupe où le document a été posté uniquement à partir du texte.\n",
        "\n",
        "Une première étape de mise en forme des données sous forme vectorielle a été réalisée et utilise une technique de \"sac de mots\" (\"bag of words\" en anglais). Les données sont de grande taille.\n",
        "\n",
        "Calculer le nombre moyen de valeurs non nulles par échantillon: qu'en déduisez-vous sur la structure des données?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNzAiMIg1joH"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "Chargez les données visualisez quelques exemples. Calculez le nombre moyen de valeurs non nulles par échantillon.\n",
        "\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1UEp2Ewi1joL",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "import scipy\n",
        "from sklearn import datasets\n",
        "\n",
        "X_ori, y_ori = datasets.fetch_20newsgroups_vectorized(subset=\"all\", return_X_y=True)\n",
        "\n",
        "## Lecture des données\n",
        "#class0 = 3 #'comp.sys.ibm.pc.hardware',\n",
        "#class1 = 4 #'comp.sys.mac.hardware'\n",
        "\n",
        "class0 = 13 # 'sci.med'\n",
        "class1 = 14 # 'sci.space'\n",
        "\n",
        "X0 = X_ori[y_ori == class0,:]\n",
        "X1 = X_ori[y_ori == class1,:]\n",
        "\n",
        "y_all = np.array([0]*X0.shape[0] + [1]*X1.shape[0])\n",
        "X_all = scipy.sparse.vstack((X0,X1))\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_all, y_all,\n",
        "                                                    random_state=42,\n",
        "                                                    test_size=0.3)\n",
        "\n",
        "print(\"Les données sont de dimension {:d}\".format(X_train.shape[1]))\n",
        "print(\"Le nombre de données d'apprentissage est {:d}\".format(y_train.shape[0]))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUdF01eCnG1u"
      },
      "outputs": [],
      "source": [
        "# Analyse des données\n",
        "#######################\n",
        "# METTRE VOTRE CODE ICI\n",
        "\n",
        "\n",
        "# Calcul du nombre moyens d'éléments non nuls\n",
        "non_nul_moyen = 4568741\n",
        "print(\"Le nombre moyen de valeurs non nulles par échantillon est {:.1f}\".format(non_nul_moyen))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9wP0Qq2F1joX"
      },
      "source": [
        "### Bayésien naïf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uVikg4Us1joZ"
      },
      "source": [
        "\n",
        "Mise en oeuvre du bayésien naïf.\n",
        "\n",
        "Ici, les représentations étant des histogrammes de fréquence de mots, les modèles de probabilité par dimension sont soit des Bernoulli (présence/absence d'une caractéristique), soit des lois multinomiales (nombre d'occurrences de mots)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hO4806Tr1jod",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "from sklearn.naive_bayes import BernoulliNB, GaussianNB, MultinomialNB\n",
        "\n",
        "clf = BernoulliNB()\n",
        "#clf = MultinomialNB()\n",
        "\n",
        "clf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Bayésien naïf: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdV-DgGWIgDN"
      },
      "source": [
        "### Prédicteurs discriminants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4H6sBUcUIgNq"
      },
      "outputs": [],
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.svm import LinearSVC\n",
        "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
        "\n",
        "clf = LogisticRegression(random_state=0, solver='lbfgs', penalty='none').fit(X_train, y_train)\n",
        "print(\"Régression logisitique: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n",
        "# Ici, le SVM n'est pas calculé dans le dual (on verra ce que ça veut dire au cours N°4)\n",
        "clf=LinearSVC(random_state=0, tol=1e-5, dual=False, C=1).fit(X_train, y_train)\n",
        "print(\"SVM linéaire: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n",
        "# Pour des données de très grande dimension, le calcul directe de la direction discriminante\n",
        "# (par estimation de la covariance ou des valeurs propres) est trop calculatoire\n",
        "#clf = LinearDiscriminantAnalysis()\n",
        "#clf.fit(X_train.toarray(), y_train)\n",
        "#print(\"Analyse discriminante: L'erreur de prédiction est {:.2f}%\".format(100*(1-clf.score(X_test, y_test))))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QJl5977rNwcT"
      },
      "source": [
        "Pour ce type de données de grande taille mais creuses (beaucoup d'attributs nuls) le bayésien naïf est une solution simple et efficace."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}